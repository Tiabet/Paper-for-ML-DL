#전체주제
Transformer 모델 소개
RNN, CNN 보다 훨씬 효율적이고 고성능
attention mechanism allows it to better capture long-range dependencies in the input sequence. 
-> Attention Mecahnism 이란? + long-range dependencies?

#Introduction
RNN 등은 NLP에서 좋은 성과를 냈지만, 한계가 존재했음. (parallelization, long-range dependencies)
parallelization : 동시에 여러 작업을 할 수 있는 능력
long-range dependencies : 길이가 긴 문장, 문서에 대한 능력.
Transformers : attention mechanism 만을 사용하는 새로운 아키텍쳐
기계번역, parallelize 가 더 좋음

#background
CNN을 사용한 Extended Neural GPU, ByteNet, ConvS2S : parallelize 등을 시도한 모델들 예시라서 참고한 듯
RNN 등에	서 conjunction 정도로만 쓰인 Attention Mechanism에 집중 -> self-attention

#Model Architecture
Encoder - Decoder : 6개의 layer, 2 - 3 개 씩의 sublayer.
6번의 layer를 거치면서 weight가 계속 수정 (feed-forward network, 이 과정에서 ReLU (음수면 0, 양수면 그대) 적용)
Encoder 와 Decoder에도 각각 multihead attention mechanism을 적용하는 sublayer가 있음.
decoder 에는 Masked Mult-head attention layer가 추가적으로 있음

요약 : Encoder는 정해진 길이 단위로 작업을 수행하면서 context를 분석, Decoder는 encoder의 아웃풋을 토대로 토큰화 진

feed-forward neural networks : 한 뱡향으로 쭉 흘러가는 network, 흘러가면서 한 층의 아웃풋은 다음 층의 인풋이 됨.

self-attention
Attention mechanishm : 특정 단어에 초점을 맞추어서 바라보면서 정확도를 따져가던 mechanism
Self-Attention Mechanism : 모든 단어, 모든 위치에 대해 weight를 바꾸어 Attention을 주면서 long-range 에 큰 효과를 보게 됨

Scaled Dot-Product Attention
소프트맥스 함수, 	차원의 루트로 나눠주는 것이 핵심!
Queries, Keys, Values 를 벡터화해서 잘 내적한 다음 차원의 크기를 일종의 패널티로 넣어주고 소프트맥스 함수 적용
Query : t 시점의 디코더 셀에서의 은닉 상태
Keys : 모든 시점의 인코더 셀의 은닉 상태들
Values : 모든 시점의 인코더 셀의 은닉 상태들


Multi-Head Attention
위의 Scaled Dot-product attention을 적용하기 전, 모든 스케일에 대해서 Key, Value, Query를 다 계산한 뒤 다 더
효과 : 다른 위치의 subspace들로부터 정보를 빼낼 수 있기 때문에, 결과적으로 모델의 성능을 발전시킴 -> long-range dependency와 연결되는 부분인듯

Positional Encoding
RNN, CNN 의 recurrence, convolution이 없기 때문에 sequence의 순서를 따질 필요가 있음.
그래서 encoder와 decoder 맨 처음에 positional encoding을 진행.
sin 함수와 cos 함수를 사용해서 위치를 알려주는 토큰을 삽입한다고 볼 수 있음.
이 작업은 recurrent, convolutional 한 작업들에 비해 엄청나게 빨라서 효율적임.

Self Attention 이 좋은 이유 셋 :
각 층마다의 계산량이 아주 적다.
계산을 동시에 할 수 있다.
먼 거리에 있는 단어, 문맥의 상관성도 쉽게 찾을 수 있다.

https://tiabet0929.tistory.com/manage/newpost/?type=post&returnURL=%2Fmanage%2Fposts%2F
